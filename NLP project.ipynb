{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity Recognition Using NLP and Word Frequency with Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The epic high fantasy movie Lord of the Rings has touched the hearts of many- and the scripts of the three movies have played a great part in the trilogy's success. This project aims to better understand the scripts by utilizing Natural Language Processing (NLP), which is the automatic manipulation of natural languages, such as speech and text, by software (Brownlee, 2019).\n",
    "\n",
    "This project focuses on two parts,  Entity classification using NLP, as well as utilizing Counter to identify word frequencies in the script. This project utilizes database ‘lotr_scripts.csv’ downloaded from https://www.kaggle.com/paultimothymooney/lord-of-the-rings-data. \n",
    "\n",
    "For the first part of this project, we attempt to exercise entity classification only on the dialogues, as they contain sentences with words that could be further classified. In this exercise, we will also utilize SpaCy, an open-surce software library for advanced natural language processing written in Python (Choi et al., 2015). SpaCy uses artificial neural networks to train its pipeline model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /Users/callistastephineyu/opt/anaconda3/lib/python3.8/site-packages (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/callistastephineyu/opt/anaconda3/lib/python3.8/site-packages (from spacy) (2.11.2)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /Users/callistastephineyu/opt/anaconda3/lib/python3.8/site-packages (from spacy) (0.6.1)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/callistastephineyu/opt/anaconda3/lib/python3.8/site-packages (from spacy) (1.0.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/callistastephineyu/opt/anaconda3/lib/python3.8/site-packages (from spacy) (1.19.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/callistastephineyu/opt/anaconda3/lib/python3.8/site-packages (from spacy) (1.0.6)\n",
      "Requirement already satisfied: setuptools in /Users/callistastephineyu/opt/anaconda3/lib/python3.8/site-packages (from spacy) (50.3.1.post20201107)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/callistastephineyu/opt/anaconda3/lib/python3.8/site-packages (from spacy) (20.4)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/callistastephineyu/opt/anaconda3/lib/python3.8/site-packages (from spacy) (2.0.6)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/callistastephineyu/opt/anaconda3/lib/python3.8/site-packages (from spacy) (2.0.6)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /Users/callistastephineyu/opt/anaconda3/lib/python3.8/site-packages (from spacy) (8.0.13)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /Users/callistastephineyu/opt/anaconda3/lib/python3.8/site-packages (from spacy) (2.4.2)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /Users/callistastephineyu/opt/anaconda3/lib/python3.8/site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/callistastephineyu/opt/anaconda3/lib/python3.8/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /Users/callistastephineyu/opt/anaconda3/lib/python3.8/site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/callistastephineyu/opt/anaconda3/lib/python3.8/site-packages (from spacy) (2.24.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/callistastephineyu/opt/anaconda3/lib/python3.8/site-packages (from spacy) (3.0.6)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /Users/callistastephineyu/opt/anaconda3/lib/python3.8/site-packages (from spacy) (0.7.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/callistastephineyu/opt/anaconda3/lib/python3.8/site-packages (from spacy) (4.50.2)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /Users/callistastephineyu/opt/anaconda3/lib/python3.8/site-packages (from spacy) (0.4.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /Users/callistastephineyu/opt/anaconda3/lib/python3.8/site-packages (from spacy) (1.8.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/callistastephineyu/opt/anaconda3/lib/python3.8/site-packages (from jinja2->spacy) (1.1.1)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /Users/callistastephineyu/opt/anaconda3/lib/python3.8/site-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/callistastephineyu/opt/anaconda3/lib/python3.8/site-packages (from packaging>=20.0->spacy) (2.4.7)\n",
      "Requirement already satisfied: six in /Users/callistastephineyu/opt/anaconda3/lib/python3.8/site-packages (from packaging>=20.0->spacy) (1.15.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/callistastephineyu/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.25.11)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/callistastephineyu/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/callistastephineyu/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/callistastephineyu/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/callistastephineyu/opt/anaconda3/lib/python3.8/site-packages (from typer<0.5.0,>=0.3.0->spacy) (7.1.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/callistastephineyu/opt/anaconda3/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy) (3.7.4.3)\n"
     ]
    }
   ],
   "source": [
    "#Installing spaCy\n",
    "!pip3 install spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One such trained pipeline model we will use is ‘en_core_web_sm’,  an english trained pipeline model optimized for CPU. The trained pipeline model contains pipelines such as tok2vec, tagger, parser, attribute_ruler, lemmatizer, and ner. Hence, the trained pipeline model contributes to the convenience of exercising NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en-core-web-sm==3.2.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.2.0/en_core_web_sm-3.2.0-py3-none-any.whl#egg=en_core_web_sm==3.2.0 in /Users/callistastephineyu/opt/anaconda3/lib/python3.8/site-packages (3.2.0)\n",
      "Requirement already satisfied: spacy<3.3.0,>=3.2.0 in /Users/callistastephineyu/opt/anaconda3/lib/python3.8/site-packages (from en-core-web-sm==3.2.0) (3.2.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/callistastephineyu/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.6)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /Users/callistastephineyu/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.9.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/callistastephineyu/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /Users/callistastephineyu/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.8)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/callistastephineyu/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.19.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/callistastephineyu/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.24.0)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /Users/callistastephineyu/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.6.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/callistastephineyu/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (20.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/callistastephineyu/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.50.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/callistastephineyu/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.3.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/callistastephineyu/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.6)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /Users/callistastephineyu/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.13)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /Users/callistastephineyu/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.7.5)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/callistastephineyu/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: jinja2 in /Users/callistastephineyu/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.11.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/callistastephineyu/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: setuptools in /Users/callistastephineyu/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (50.3.1.post20201107)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /Users/callistastephineyu/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /Users/callistastephineyu/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.8.2)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /Users/callistastephineyu/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/callistastephineyu/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.25.11)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/callistastephineyu/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/callistastephineyu/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/callistastephineyu/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2020.6.20)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /Users/callistastephineyu/opt/anaconda3/lib/python3.8/site-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (5.2.1)\n",
      "Requirement already satisfied: six in /Users/callistastephineyu/opt/anaconda3/lib/python3.8/site-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.15.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/callistastephineyu/opt/anaconda3/lib/python3.8/site-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.7)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/callistastephineyu/opt/anaconda3/lib/python3.8/site-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.1.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/callistastephineyu/opt/anaconda3/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.7.4.3)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/callistastephineyu/opt/anaconda3/lib/python3.8/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (7.1.2)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "#import the necessary libraries and packages\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "#download the trained pipeline model\n",
    "! python -m spacy download en_core_web_sm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using Pandas ot read the CSV file for visualisation purposes only.\n",
    "lotr_visualisation = pd.read_csv('lotr_scripts.csv')\n",
    "lotr_visualisation.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The database shown above contains the characters and their lines as well as the movie the lines are from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Oh Smeagol Ive got one! , Ive got a fish Smeag...\n",
       "1       Pull it in! Go on, go on, go on, pull it in!  \n",
       "2                                             Arrghh! \n",
       "3                                            Deagol!  \n",
       "4                                            Deagol!  \n",
       "Name: dialog, dtype: object"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lotr = pd.read_csv(\"lotr_scripts.csv\")\n",
    "lotr = lotr['dialog']\n",
    "#showcasing the first 5 dialogues in the database\n",
    "lotr.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using the trained pipeline model 'en_core_web_sm'\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Part - Entity Classification using NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Some useful functions to be called for entity classification are the nlp() function and  (available through SpaCy) and the .ents attribute. The nlp() function effortlessly tokenizes the lines, preparing it for entity recognition. The .ents attribute then narrows the lines down to just words or phrases they consider as entities. To better visualize the lines alongside the recognized entities, we shall make use of displaCy, an open-source named entity visualizer from spaCy that supports NLP rendering. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two lines from the database are of our main focus. The first sentence, labelled sent1, is as displayed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    My dear Sam, you cannot always be torn in two. , You will     have to be one and whole for many years. You have so much to enjoy and to be     and to do. Your part in the story will go on.  \n"
     ]
    }
   ],
   "source": [
    "#first sentence to be analyzed\n",
    "sent1 = nlp(lotr[100])\n",
    "print(sent1.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Sam, two, many years)\n"
     ]
    }
   ],
   "source": [
    "#printing the entities in the 101st line from the movie 'Return of the Kings'\n",
    "print(sent1.ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">    My dear \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Sam\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       ", you cannot always be torn in \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    two\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       ". , You will     have to be one and whole for \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    many years\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ". You have so much to enjoy and to be     and to do. Your part in the story will go on.  </div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#visualizing displacy for sent1\n",
    "displacy.render(sent1, style= 'ent' , jupyter = True, options= {'distance':100})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same exercise is done on the 40th line in the database, shown as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ,And thus     it was a Fourth Age of Middle Earth began. And the Fellowship of the Ring,     though eternally bound by friendship and love was ended.            Thirteen months to the day since Gandalf sent us on our long journey we find     ourselves looking upon a familiar sight.  \n",
      "(a Fourth Age, Middle Earth, the Fellowship of the Ring, Thirteen months to the day)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">    ,And thus     it was \n",
       "<mark class=\"entity\" style=\"background: #ffeb80; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    a Fourth Age\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">EVENT</span>\n",
       "</mark>\n",
       " of \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Middle Earth\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       " began. And \n",
       "<mark class=\"entity\" style=\"background: #ff8197; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the Fellowship of the Ring\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LAW</span>\n",
       "</mark>\n",
       ",     though eternally bound by friendship and love was ended.            \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Thirteen months to the day\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " since Gandalf sent us on our long journey we find     ourselves looking upon a familiar sight.  </div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#our second sentence\n",
    "sent2 = nlp(lotr[39])\n",
    "print(sent2.text)\n",
    "print(sent2.ents)\n",
    "displacy.render(sent2, style= 'ent' , jupyter = True, options= {'distance':100})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What have we got?\n",
    "\n",
    "It seems that more entities are identified in this line. We shall use this line for further analysis in the next part. \n",
    "\n",
    "Another common way to exercise NLP is through the usage of  Natural Language ToolKit (NLTK). It is a suite of libraries and programs for symbolic and statistical NLP for English written in Python.\n",
    "For this project, the pipelines used are:  ‘punkt’, ‘average_perceptron_tagger’, ‘maxent_ne_chunker’. 'punkt' is a tokenizer which divides a text into a list of sentences, and is trained using an unsupervised algorithm to build a model for abbreviation words, collocations, and words that start sentences <https://www.kite.com/python/docs/nltk.punkt>. 'average_perceptron_tagger' is one of many taggers available in the NLTK library. It is used to tag words with their parts of speech (POS). 'maxent_ne_chunker' is a chunker tool that contains two pre-trained English named entity chunkers trained on an ACE corpus. Chunking is an alternative to parsing that provides a partial syntactic structure of a sentence, with a limited tree depth, as opposed to full on parsing. Despite being more limited than parsing, it provides a more robust process and is sufficient for this project. \n",
    "\n",
    "A key step in NLP is preprocessing. Preprocessing is a method to clean the text data and make it ready to feed data to the model (Agrawal, 2021). In this project, preprocessing includes using word_tokenize to tokenize the sentences into words, as well as attaching tags using pos_tag to these tokenized words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/callistastephineyu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/callistastephineyu/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/callistastephineyu/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#using nltk\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', ','),\n",
       " ('And', 'CC'),\n",
       " ('thus', 'RB'),\n",
       " ('it', 'PRP'),\n",
       " ('was', 'VBD'),\n",
       " ('a', 'DT'),\n",
       " ('Fourth', 'NNP'),\n",
       " ('Age', 'NNP'),\n",
       " ('of', 'IN'),\n",
       " ('Middle', 'NNP'),\n",
       " ('Earth', 'NNP'),\n",
       " ('began', 'VBD'),\n",
       " ('.', '.'),\n",
       " ('And', 'CC'),\n",
       " ('the', 'DT'),\n",
       " ('Fellowship', 'NNP'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('Ring', 'NNP'),\n",
       " (',', ','),\n",
       " ('though', 'IN'),\n",
       " ('eternally', 'RB'),\n",
       " ('bound', 'VBN'),\n",
       " ('by', 'IN'),\n",
       " ('friendship', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('love', 'NN'),\n",
       " ('was', 'VBD'),\n",
       " ('ended', 'VBN'),\n",
       " ('.', '.'),\n",
       " ('Thirteen', 'JJ'),\n",
       " ('months', 'NNS'),\n",
       " ('to', 'TO'),\n",
       " ('the', 'DT'),\n",
       " ('day', 'NN'),\n",
       " ('since', 'IN'),\n",
       " ('Gandalf', 'NNP'),\n",
       " ('sent', 'VBD'),\n",
       " ('us', 'PRP'),\n",
       " ('on', 'IN'),\n",
       " ('our', 'PRP$'),\n",
       " ('long', 'JJ'),\n",
       " ('journey', 'NN'),\n",
       " ('we', 'PRP'),\n",
       " ('find', 'VBP'),\n",
       " ('ourselves', 'PRP'),\n",
       " ('looking', 'VBG'),\n",
       " ('upon', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('familiar', 'JJ'),\n",
       " ('sight', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###preprocessing the text\n",
    "\n",
    "#creating a function for preprocessing\n",
    "def preprocess_sent(sent):\n",
    "    text = word_tokenize(sent)\n",
    "    tags = pos_tag(text)\n",
    "    return tags\n",
    "\n",
    "sent2_preprocessed = preprocess_sent(sent2.text)\n",
    "#to display the preprocessed sentence 2\n",
    "sent2_preprocessed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do we have here? it is a list of tuples containing the individual words in the sentence and their corresponding parts of speech (POS). Now that the lines have been preprocessed, it is ready to be chunked. The output of the chunking process will be entities recognized, along with their labels from the NLTK library.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  ,/,\n",
      "  And/CC\n",
      "  thus/RB\n",
      "  it/PRP\n",
      "  was/VBD\n",
      "  a/DT\n",
      "  (ORGANIZATION Fourth/NNP Age/NNP)\n",
      "  of/IN\n",
      "  (GPE Middle/NNP Earth/NNP)\n",
      "  began/VBD\n",
      "  ./.\n",
      "  And/CC\n",
      "  the/DT\n",
      "  (ORGANIZATION Fellowship/NNP)\n",
      "  of/IN\n",
      "  the/DT\n",
      "  Ring/NNP\n",
      "  ,/,\n",
      "  though/IN\n",
      "  eternally/RB\n",
      "  bound/VBN\n",
      "  by/IN\n",
      "  friendship/NN\n",
      "  and/CC\n",
      "  love/NN\n",
      "  was/VBD\n",
      "  ended/VBN\n",
      "  ./.\n",
      "  Thirteen/JJ\n",
      "  months/NNS\n",
      "  to/TO\n",
      "  the/DT\n",
      "  day/NN\n",
      "  since/IN\n",
      "  (PERSON Gandalf/NNP)\n",
      "  sent/VBD\n",
      "  us/PRP\n",
      "  on/IN\n",
      "  our/PRP$\n",
      "  long/JJ\n",
      "  journey/NN\n",
      "  we/PRP\n",
      "  find/VBP\n",
      "  ourselves/PRP\n",
      "  looking/VBG\n",
      "  upon/IN\n",
      "  a/DT\n",
      "  familiar/JJ\n",
      "  sight/NN\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "#named entities\n",
    "named_entities = nltk.ne_chunk(sent2_preprocessed)\n",
    "print(named_entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the conclusion here?\n",
    "We can see that the Fellowship is recognised as an organisation. Middle Earth is also rightfully recognised as a location.  The NLTK POS tag 'NNP' indicates a proper noun. The definition of the other tags displayed above can easily be found in https://www.guru99.com/pos-tagging-chunking-nltk.html. \n",
    "The entity recognition process done in spaCy seem to provide a different output as compared to NLTK. Why is that so? To answer this question,we need to first understand the way these two libraries were built. NLTK is a string processing library, where each function takes strings as input and returns a processed string. In contrast, spaCy takes on an object-oriented approach. Each function returns objects instead of strings or arrays (Kakarla, 2019). In terms of usability, both seem to get the job done well!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Part - Word Frequency with Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we find ourselves in the second part of this project, which is to identify the character that spoke the most lines throughout the three movies. We will then see if the character that speaks the most is the main character or not.\n",
    "\n",
    "To do this, the CSV file is read by pandas for the third time, this time only considering the ‘char’ column in the database. This column contains the characters that spoke the lines in the movies. Hence, duplicates will surely be present, and is to be counted through the Counter() function in python to find the characters that spoke the most lines.\n",
    "\n",
    "To compile all the characters in the column, the list ‘characters’ is created, as the Counter() function only applies to lists and dictionaries. The most_common attribute is used to find the most common word that occurs along the ‘char’ column. The top 10 most common words are then displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('FRODO', 225), ('SAM', 216), ('GANDALF', 204), ('ARAGORN', 185), ('PIPPIN', 163), ('MERRY', 137), ('GOLLUM', 133), ('GIMLI', 116), ('THEODEN', 110), ('FARAMIR', 65)]\n"
     ]
    }
   ],
   "source": [
    "#loading the LOTR script CSV file again, this time only extracting the characters and not their lines. \n",
    "lotr2 = pd.read_csv(\"lotr_scripts.csv\")\n",
    "lotr2 = lotr2['char']\n",
    "lotr2.head(5)\n",
    "characters = [char for char in lotr2]\n",
    "count = Counter(characters)\n",
    "top_ten = count.most_common(10)\n",
    "print(top_ten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the character ‘Frodo’ speaks the most lines (speaking 225 times throughout the three movies), followed by his best friend/gardener Sam, and Gandalf the wizard. \n",
    "\n",
    "Frodo may be the main character in the Lord of the Rings, but which characters are mentioned the most in the dialogues of the three movies? To answer this, we shall use some features from the NLTK library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Oh Smeagol Ive got one! , Ive got a fish Smeagol, Smeagol!    ', 'Pull it in! Go on, go on, go on, pull it in! \\xa0', 'Arrghh! ', 'Deagol! \\xa0', 'Deagol! \\xa0']\n"
     ]
    }
   ],
   "source": [
    "#which characters are mentioned the most throughout the three movies?\n",
    "script = [line for line in lotr]\n",
    "#to demonstrate what script is:\n",
    "print(script[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also keep in mind that the trilogy scripts contain stop words such as “a”, “the”, “is”, “are” and etc., words that we do not want to appear in the most common words list. Hence, from the NLTK library we import the ‘stopwords’ package. RegexpTokenizer is also imported as we want to split a string into substrings according to the code that we specify. In this exercise, we specify the tokenizer as ‘r\\w+’ for RegexpTokenizer to split the words in sentences apart. We then get a list of strings of individual words instead of sentences. \n",
    "\n",
    "To further clean the tokenized list, we shall then create a new list called words_clean that contains all the tokenized words that are not stop words. The hard space or no-break space ‘xa0’ is also removed such that the words_clean list is free from unwanted symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/callistastephineyu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Importing stop words and Regexp for punctuation removal\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "stop_words = nltk.corpus.stopwords.words(\"english\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Oh', 'Smeagol', 'Ive', 'got', 'one', 'Ive', 'got', 'a', 'fish', 'Smeagol', 'Smeagol', 'Pull', 'it', 'in', 'Go', 'on', 'go', 'on', 'go', 'on']\n"
     ]
    }
   ],
   "source": [
    "#tokenizing the script\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokenized = tokenizer.tokenize(str(script))\n",
    "print(tokenized[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Oh', 'Smeagol', 'Ive', 'got', 'one', 'Ive', 'got', 'fish', 'Smeagol', 'Smeagol', 'Pull', 'Go', 'go', 'go', 'pull', 'Arrghh', 'Deagol', 'Deagol', 'Deagol', 'Give']\n"
     ]
    }
   ],
   "source": [
    "#cleaning words\n",
    "# 'xa0' seems to appear quite often in the tokenized list. \n",
    "#It actually represents a hard space or a no-break space in a program. A simple way to remove it is as follows: \n",
    "words_clean = [word for word in tokenized if word.lower() not in stop_words and word !='xa0']\n",
    "print(words_clean[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As demonstrated, the stop words are gone from the tokenized list! \n",
    "\n",
    "Again, the Counter() function is used to count the number of occurences of words in words_clean. The top ten words are shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Frodo', 147), ('us', 110), ('one', 87), ('go', 86), ('must', 83), ('come', 76), ('Gandalf', 68), ('would', 68), ('back', 67), ('know', 67)]\n"
     ]
    }
   ],
   "source": [
    "count2 = Counter(words_clean)\n",
    "top_ten_words = count2.most_common(10)\n",
    "print(top_ten_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapping up\n",
    "\n",
    "Interesting! The word that is spoken the most throughout the trilogy is ‘Frodo’! His name is mentioned 147 times across the three movies. It seems that Frodo really is significant in the movie. Gandalf’s name seems to be called more than Sam’s though. I wonder why that is...? You may infer the answer by reading the Lord of the Rings character analysis in this website: https://literariness.org/2021/02/18/the-lord-of-the-rings-character-analysis/\n",
    "\n",
    "By now, we have successfully performed entity recognition and word frequency counting. These are just two simple examples of the many things we can achieve through Natural Language Processing! \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "Choi et al. 2015. It Depends: Dependency Parser Comparison Using A Web-based Evaluation Tool.\n",
    "\n",
    "Agrawal, R., 2021. Must known techniques for text preprocessing in NLP. Analytics Vidhya. Available at: https://www.analyticsvidhya.com/blog/2021/06/must-known-techniques-for-text-preprocessing-in-nlp/ [Accessed January 18, 2022]. \n",
    "\n",
    "Brownlee, J., 2019. What is natural language processing? Machine Learning Mastery. Available at: https://machinelearningmastery.com/natural-language-processing/ [Accessed January 18, 2022]. \n",
    "\n",
    "Kakarla, S., 2021. Natural language processing: NLTK vs spacy. ActiveState. Available at: https://www.activestate.com/blog/natural-language-processing-nltk-vs-spacy/ [Accessed January 18, 2022]. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
